{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is for the Models of our Project.\n",
    "\n",
    "Models include:\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- KNN\n",
    "- Neural Network\n",
    "\n",
    "Below you can find the Pre-processing, Training, and Testing for Each model\n",
    "\n",
    "At the end we will conclude with a comparison between each model and discuss results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection\n",
    "data = pd.read_csv('credit_card_fraud.csv', parse_dates=['trans_date_trans_time',])\n",
    "\n",
    "X = data.drop(['is_fraud'], axis=1)\n",
    "Y = data['is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing --------------------------------------------------------\n",
    "\n",
    "# changing data types\n",
    "X['dob'] = pd.to_datetime(X['dob'])\n",
    "\n",
    "# creating columns out of our original Dataset --------------------------\n",
    "\n",
    "X['hour_of_transaction'] = X.trans_date_trans_time.dt.hour # hour of transaction\n",
    "X['month_of_transaction'] = X.trans_date_trans_time.dt.month # month of transaction\n",
    "X['dow_of_transaction'] = X.trans_date_trans_time.dt.day_name() # day of week of transaction\n",
    "X['cust_age'] = (X['trans_date_trans_time'] - X['dob']).astype('timedelta64[Y]') # age of person during transaction\n",
    "\n",
    "# encoding: 0 = normal time, 1 = odd time\n",
    "X['Normal_transaction_time'] = 0\n",
    "X.loc[X.hour_of_transaction < 5,'Normal_transaction_time'] = 1\n",
    "X.loc[X.hour_of_transaction > 21,'Normal_transaction_time'] = 1\n",
    "\n",
    "# one-hot encoding the categorical features\n",
    "encoder = OneHotEncoder()\n",
    "dow_encoded = encoder.fit_transform(X[['dow_of_transaction']])\n",
    "dow_encoded_df = pd.DataFrame(dow_encoded.toarray(), columns=encoder.categories_[0])\n",
    "X = pd.concat([X, dow_encoded_df], axis=1)\n",
    "\n",
    "state_encoded = encoder.fit_transform(X[['state']])\n",
    "state_encoded_df = pd.DataFrame(state_encoded.toarray(), columns=encoder.categories_[0])\n",
    "X = pd.concat([X,state_encoded_df], axis=1)\n",
    "\n",
    "merch_encoded = encoder.fit_transform(X[['merchant']])\n",
    "merch_encoded_df = pd.DataFrame(merch_encoded.toarray(), columns=encoder.categories_[0])\n",
    "X = pd.concat([X, merch_encoded_df], axis=1)\n",
    "\n",
    "cat_encoded = encoder.fit_transform(X[['category']])\n",
    "cat_encoded_df = pd.DataFrame(cat_encoded.toarray(), columns=encoder.categories_[0])\n",
    "X = pd.concat([X, cat_encoded_df], axis=1)\n",
    "\n",
    "city_encoded = encoder.fit_transform(X[['city']])\n",
    "city_encoded_df = pd.DataFrame(city_encoded.toarray(), columns=encoder.categories_[0])\n",
    "X = pd.concat([X, city_encoded_df], axis=1)\n",
    "\n",
    "\n",
    "# Normalizing the features with varying features ------------------------------------------------------------\n",
    "\n",
    "# min-max normalization since no real outliers for these features\n",
    "X['cust_age'] = (X['cust_age'] - X['cust_age'].min()) / (X['cust_age'].max() - X['cust_age'].min())\n",
    "\n",
    "# z-score normalization for values that are wide-spread such as amt and city population\n",
    "X['amt'] = (X['amt'] - X['amt'].mean()) / X['amt'].std() \n",
    "X['city_pop'] = (X['city_pop'] - X['city_pop'].mean()) / X['city_pop'].std() \n",
    "\n",
    "# getting rid of unnecessary columns\n",
    "X.drop(['trans_num', 'job','trans_date_trans_time', 'state', 'city', 'merchant', 'category', 'dow_of_transaction', 'dob'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this data set is heavely skewed in Non-Fraudulent transactions favor, we have done some research in how to address this.\n",
    "We concluded that we can take the approach of doing under-sampling, over-sampling, and combining both.\n",
    "\n",
    "Under-sampling: The number of samples taken from majority class (Not Fraud) will be equal to total number of samples of minority class (Fraud)\n",
    "Over-sampling: Selecting random samples from the minority class (Fraud) and adding to the training data copies of the sample\n",
    "\n",
    "\n",
    "Logistic Regression Model - Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape   :  (2851, 913)\n",
      "Training Labels Shape :  (2851,)\n",
      "Testing Data Shape    :  (713, 913)\n",
      "Testing Labels Shape  :  (713,)\n",
      "\n",
      "Logistic Regression Results with Under-Sampling:\n",
      "\n",
      "Training Accuracy :  0.8905647141353911\n",
      "Testing  Accuracy :  0.879382889200561\n",
      "Training Set f1 score :  0.8925619834710744\n",
      "Testing  Set f1 score :  0.8753623188405797\n",
      "\n",
      "Test set precision :  0.8894989704873026\n",
      "Test set recall    :  0.9014925373134328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs484/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "under_sample = RandomUnderSampler()\n",
    "X_under, Y_under = under_sample.fit_resample(X,Y) # data set used for all under sampled models\n",
    "\n",
    "X_train_u, X_test_u, Y_train_u, Y_test_u = train_test_split(X_under, Y_under, test_size = 0.2, random_state=42)\n",
    "\n",
    "print('Training Data Shape   : ', X_train_u.shape)\n",
    "print('Training Labels Shape : ', Y_train_u.shape)\n",
    "print('Testing Data Shape    : ', X_test_u.shape)\n",
    "print('Testing Labels Shape  : ', Y_test_u.shape)\n",
    "print()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train_u,Y_train_u)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "pred_train_lr = lr_model.predict(X_train_u)\n",
    "pred_test_lr  = lr_model.predict(X_test_u)\n",
    "\n",
    "print('Logistic Regression Results with Under-Sampling:')\n",
    "print()\n",
    "print('Training Accuracy : ', accuracy_score(Y_train_u, pred_train_lr))\n",
    "print('Testing  Accuracy : ', accuracy_score(Y_test_u, pred_test_lr))\n",
    "\n",
    "# Checking f1 score, precision and recall\n",
    "print('Training Set f1 score : ', f1_score(Y_train_u, pred_train_lr))\n",
    "print('Testing  Set f1 score : ', f1_score(Y_test_u, pred_test_lr))\n",
    "print()\n",
    "print('Test set precision : ', precision_score(Y_train_u, pred_train_lr))\n",
    "print('Test set recall    : ', recall_score(Y_test_u, pred_test_lr))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model - Under Sampling\n",
    "\n",
    "Hyperparameters include:\n",
    "n_estimators: Determines the number of decision tress that are \"grown\" in random forest\n",
    "max_depth: the maximum depth for each decision tree\n",
    "random_state: helps randomize the data to generate diverse decision trees and will help in comparing later since each model has same random_state\n",
    "\n",
    "Hyperparameters tested:\n",
    "\n",
    "n_estimators=100, max_depth=10, random_state=42\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Results with Under-Sampling:\n",
      "\n",
      "Training Set Accuracy :  1.0\n",
      "Testing Set Accuracy  :  0.9635343618513323\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=200, max_depth=200, random_state=42)\n",
    "rf_classifier.fit(X_train_u, Y_train_u)\n",
    "\n",
    "pred_train_rf = rf_classifier.predict(X_train_u)\n",
    "pred_test_rf = rf_classifier.predict(X_test_u)\n",
    "\n",
    "print('Random Forest Classifier Results with Under-Sampling:')\n",
    "print()\n",
    "\n",
    "print('Training Set Accuracy : ', accuracy_score(Y_train_u, pred_train_rf))\n",
    "print('Testing Set Accuracy  : ', accuracy_score(Y_test_u, pred_test_rf))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network (MLP) Classifier Results with Under-Sampling:\n",
      "\n",
      "Training Set Accuracy :  0.9817607856892319\n",
      "Testing Set Accuracy  :  0.8611500701262272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(913,500,250,100,50,1), activation='relu', random_state=42)\n",
    "nn_classifier.fit(X_train_u, Y_train_u)\n",
    "\n",
    "pred_train_nn = nn_classifier.predict(X_train_u)\n",
    "pred_test_nn = nn_classifier.predict(X_test_u)\n",
    "\n",
    "print('Neural Network (MLP) Classifier Results with Under-Sampling:')\n",
    "print()\n",
    "\n",
    "print('Training Set Accuracy : ', accuracy_score(Y_train_u, pred_train_nn))\n",
    "print('Testing Set Accuracy  : ', accuracy_score(Y_test_u, pred_test_nn))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape   :  (540520, 913)\n",
      "Training Labels Shape :  (540520,)\n",
      "Testing Data Shape    :  (135130, 913)\n",
      "Testing Labels Shape  :  (135130,)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs484/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results with Under-Sampling:\n",
      "\n",
      "Training Accuracy :  0.8968770813290905\n",
      "Testing  Accuracy :  0.8968992821727225\n",
      "Training Set f1 score :  0.899475556004415\n",
      "Testing  Set f1 score :  0.8997467042772438\n",
      "\n",
      "Test set precision :  0.8770777206446122\n",
      "Test set recall    :  0.9240160215196795\n"
     ]
    }
   ],
   "source": [
    "over_sample = RandomOverSampler()\n",
    "X_over, Y_over = over_sample.fit_resample(X,Y) # data set used for all over sampled models\n",
    "\n",
    "\n",
    "X_train_o, X_test_o, Y_train_o, Y_test_o = train_test_split(X_over, Y_over, test_size = 0.2, random_state=42)\n",
    "\n",
    "print('Training Data Shape   : ', X_train_o.shape)\n",
    "print('Training Labels Shape : ', Y_train_o.shape)\n",
    "print('Testing Data Shape    : ', X_test_o.shape)\n",
    "print('Testing Labels Shape  : ', Y_test_o.shape)\n",
    "print()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model_over = LogisticRegression()\n",
    "lr_model_over.fit(X_train_o,Y_train_o)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "pred_train_lr2 = lr_model_over.predict(X_train_o)\n",
    "pred_test_lr2  = lr_model_over.predict(X_test_o)\n",
    "\n",
    "print('Logistic Regression Results with Under-Sampling:')\n",
    "print()\n",
    "print('Training Accuracy : ', accuracy_score(Y_train_o, pred_train_lr2))\n",
    "print('Testing  Accuracy : ', accuracy_score(Y_test_o, pred_test_lr2))\n",
    "\n",
    "# Checking f1 score, precision and recall\n",
    "print('Training Set f1 score : ', f1_score(Y_train_o, pred_train_lr2))\n",
    "print('Testing  Set f1 score : ', f1_score(Y_test_o, pred_test_lr2))\n",
    "print()\n",
    "print('Test set precision : ', precision_score(Y_train_o, pred_train_lr2))\n",
    "print('Test set recall    : ', recall_score(Y_test_o, pred_test_lr2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Results with Under-Sampling:\n",
      "\n",
      "Training Set Accuracy :  0.968252793606157\n",
      "Testing Set Accuracy  :  0.9680011840449937\n"
     ]
    }
   ],
   "source": [
    "rf_classifier_o = RandomForestClassifier(n_estimators=200, max_depth=200, random_state=42)\n",
    "rf_classifier_o.fit(X_train_o, Y_train_o)\n",
    "\n",
    "pred_train_rf2 = rf_classifier.predict(X_train_o)\n",
    "pred_test_rf2 = rf_classifier.predict(X_test_o)\n",
    "\n",
    "print('Random Forest Classifier Results with Under-Sampling:')\n",
    "print()\n",
    "\n",
    "print('Training Set Accuracy : ', accuracy_score(Y_train_o, pred_train_rf2))\n",
    "print('Testing Set Accuracy  : ', accuracy_score(Y_test_o, pred_test_rf2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs484",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
